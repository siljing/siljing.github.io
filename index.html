<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://siljing.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://siljing.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Learing_o" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/22/Learing_o/" class="article-date">
  <time class="dt-published" datetime="2024-03-22T12:25:38.997Z" itemprop="datePublished">2024-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h1><h2 id="main"><a href="#main" class="headerlink" title="main()"></a>main()</h2><h3 id="1-创建解释器，添加命令行参数。"><a href="#1-创建解释器，添加命令行参数。" class="headerlink" title="1. 创建解释器，添加命令行参数。"></a>1. 创建解释器，添加命令行参数。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(...)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<h3 id="2-设置随机数种子以及每个任务包含的的标签数量"><a href="#2-设置随机数种子以及每个任务包含的的标签数量" class="headerlink" title="2. 设置随机数种子以及每个任务包含的的标签数量"></a>2. 设置随机数种子以及每个任务包含的的标签数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set_seed(args)</span><br><span class="line">per_types = args.per_types</span><br></pre></td></tr></table></figure>
<h3 id="3-持续学习"><a href="#3-持续学习" class="headerlink" title="3. 持续学习"></a>3. 持续学习</h3><p>迭代每个任务</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step_id <span class="keyword">in</span> <span class="built_in">range</span>(args.start_step, args.nb_tasks)</span><br></pre></td></tr></table></figure>
<p>获取当前任务的标签集，标签数量和 PAD ids。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels = get_labels_dy(args.labels, per_types, step_id=step_id)</span><br><span class="line">num_labels = <span class="built_in">len</span>(labels)</span><br><span class="line">pad_token_label_id = CrossEntropyLoss().ignore_index</span><br></pre></td></tr></table></figure>
<p>设置 model_name_or_path</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果是第一轮，则加载 bert-base-uncased 模型</span></span><br><span class="line"><span class="keyword">if</span> step_id == <span class="number">0</span>:</span><br><span class="line">    model_name_or_path = <span class="string">&quot;bert-base-uncased&quot;</span> </span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># 否则加载上一轮模型</span></span><br><span class="line">    model_name_or_path = os.path.join(args.output_dir, <span class="string">&quot;task_&quot;</span> + <span class="built_in">str</span>(step_id - <span class="number">1</span>))      </span><br></pre></td></tr></table></figure>
<p>训练和评价模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_and_eval(args, labels, num_labels, pad_token_label_id, model_name_or_path,output_dir, data_dir, step_id)</span><br></pre></td></tr></table></figure>


<h1 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h1><h2 id="read-examples-from-file"><a href="#read-examples-from-file" class="headerlink" title="read_examples_from_file()"></a>read_examples_from_file()</h2><h2 id="convert-examples-to-features"><a href="#convert-examples-to-features" class="headerlink" title="convert_examples_to_features()"></a>convert_examples_to_features()</h2><h2 id="load-and-cache-examples"><a href="#load-and-cache-examples" class="headerlink" title="load_and_cache_examples()"></a>load_and_cache_examples()</h2><h3 id="1-从文件中加载样本特征"><a href="#1-从文件中加载样本特征" class="headerlink" title="1. 从文件中加载样本特征"></a>1. 从文件中加载样本特征</h3><p>如果 cached_features_file 存在，则直接从该文件中加载特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">features = torch.load(cached_features_file)</span><br></pre></td></tr></table></figure>
<p>否则先处理原始数据，再加载特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">examples = read_examples_from_file(data_dir, mode)</span><br><span class="line">features = convert_examples_to_features(...)</span><br></pre></td></tr></table></figure>
<h3 id="2-提取features的属性并构建数据集"><a href="#2-提取features的属性并构建数据集" class="headerlink" title="2. 提取features的属性并构建数据集"></a>2. 提取features的属性并构建数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">all_input_ids = torch.tensor([f.input_ids <span class="keyword">for</span> f <span class="keyword">in</span> features], dtype=torch.long)</span><br><span class="line">all_input_mask = torch.tensor([f.input_mask <span class="keyword">for</span> f <span class="keyword">in</span> features], dtype=torch.long)</span><br><span class="line">all_segment_ids = torch.tensor([f.segment_ids <span class="keyword">for</span> f <span class="keyword">in</span> features], dtype=torch.long)</span><br><span class="line">all_label_ids = torch.tensor([f.label_ids <span class="keyword">for</span> f <span class="keyword">in</span> features], dtype=torch.long)</span><br><span class="line">dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)</span><br></pre></td></tr></table></figure>
<h3 id="3-返回该数据集"><a href="#3-返回该数据集" class="headerlink" title="3. 返回该数据集"></a>3. 返回该数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>


<h1 id="工具函数"><a href="#工具函数" class="headerlink" title="工具函数"></a>工具函数</h1><h2 id="get-exemplar-means"><a href="#get-exemplar-means" class="headerlink" title="get_exemplar_means()"></a>get_exemplar_means()</h2><p><strong>计算每个类别的原型，即均值向量</strong></p>
<h3 id="1-将每个样本按标签分类"><a href="#1-将每个样本按标签分类" class="headerlink" title="1. 将每个样本按标签分类"></a>1. 将每个样本按标签分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建了一个字典，包含了所有可能的类别索引，每个类别对应一个空列表</span></span><br><span class="line">cls_exemplar = &#123;cls: [] <span class="keyword">for</span> cls <span class="keyword">in</span> <span class="built_in">range</span>(n_tags)&#125;  。</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(support_reps, support_labels):</span><br><span class="line">    <span class="comment"># 将每个样本按照标签分类存储在cls_exemplar字典中</span></span><br><span class="line">    cls_exemplar[y.item()].append(x)</span><br></pre></td></tr></table></figure>
<h3 id="2-计算每个类别的原型"><a href="#2-计算每个类别的原型" class="headerlink" title="2. 计算每个类别的原型"></a>2. 计算每个类别的原型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> cls, exemplar <span class="keyword">in</span> cls_exemplar.items():</span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> exemplar:</span><br><span class="line">        feature.data = feature.data / feature.data.norm()  <span class="comment"># Normalize  </span></span><br><span class="line">        features.append(feature)</span><br><span class="line">        <span class="comment"># 如果当前类别下没有样本，则随机初始化一个与样本表示reps大小相同的张量作为该类别的原型</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(features) == <span class="number">0</span>:</span><br><span class="line">            mu_y = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="built_in">tuple</span>(x.size())).to(args.device)</span><br><span class="line">            mu_y = mu_y.squeeze()  </span><br><span class="line">        <span class="comment"># 如果有，则计算当前类别的所有样本的reps的均值作为原型</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features = torch.stack(features) </span><br><span class="line">            mu_y = features.mean(<span class="number">0</span>).squeeze()</span><br><span class="line">        mu_y.data = mu_y.data / mu_y.data.norm()  <span class="comment"># Normalize  </span></span><br><span class="line">        exemplar_means[cls] = mu_y</span><br></pre></td></tr></table></figure>
<h3 id="3-返回包含每个类别原型的列表"><a href="#3-返回包含每个类别原型的列表" class="headerlink" title="3. 返回包含每个类别原型的列表"></a>3. 返回包含每个类别原型的列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> exemplar_means</span><br></pre></td></tr></table></figure>
<h2 id="get-support-encodings-and-labels-total"><a href="#get-support-encodings-and-labels-total" class="headerlink" title="get_support_encodings_and_labels(_total)()"></a>get_support_encodings_and_labels(_total)()</h2><p><strong>获取支持集的encodings和labels</strong></p>
<h3 id="1-获取train-loader，support-loader，support-o-loader中的encodings和labels"><a href="#1-获取train-loader，support-loader，support-o-loader中的encodings和labels" class="headerlink" title="1. 获取train_loader，support_loader，support_o_loader中的encodings和labels"></a>1. 获取train_loader，support_loader，support_o_loader中的encodings和labels</h3><p>获取train_loader中每个批次的encodings和labels(仅get_support_encodings_and_labels_total)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_iterator = tqdm(train_loader, desc=<span class="string">&quot;Support data representations&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iterator):</span><br><span class="line">    encodings, labels = get_token_encodings_and_labels(args, model, batch)</span><br><span class="line">    encodings = encodings.view(-<span class="number">1</span>, encodings.shape[-<span class="number">1</span>])</span><br><span class="line">    labels = labels.flatten()</span><br><span class="line">    <span class="comment"># 过滤掉标签为填充标记的部分</span></span><br><span class="line">    idx = torch.where((labels - pad_token_label_id) != <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    support_encodings.append(encodings[idx])</span><br><span class="line">    support_labels.append(labels[idx])</span><br></pre></td></tr></table></figure>
<p>同样的操作获取support_loader和support_o_loader中每个批次的encodings和labels </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">support_iterator = tqdm(support_loader, desc=<span class="string">&quot;Support data representations&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> index, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(support_iterator):</span><br><span class="line">        ...</span><br><span class="line">support_o_iterator = tqdm(support_o_loader, desc=<span class="string">&quot;Support data representations&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> _, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(support_o_iterator):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h3 id="2-返回所有的encodings和labels"><a href="#2-返回所有的encodings和labels" class="headerlink" title="2. 返回所有的encodings和labels"></a>2. 返回所有的encodings和labels</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.cat(support_encodings), torch.cat(support_labels)</span><br></pre></td></tr></table></figure>


<h2 id="get-token-logits-and-labels"><a href="#get-token-logits-and-labels" class="headerlink" title="get_token_logits_and_labels"></a>get_token_logits_and_labels</h2><p><strong>使用原有的预训练BERT-NER模型获取预测分数和输出标签</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = &#123;<span class="string">&quot;input_ids&quot;</span>: batch[<span class="number">0</span>], <span class="string">&quot;attention_mask&quot;</span>: batch[<span class="number">1</span>], </span><br><span class="line">                <span class="string">&quot;output_hidden_states&quot;</span>: <span class="literal">True</span>, <span class="string">&quot;mode&quot;</span>: <span class="string">&quot;dev&quot;</span>&#125;</span><br><span class="line">    <span class="keyword">if</span> model.config.model_type != <span class="string">&quot;distilbert&quot;</span>:</span><br><span class="line">        inputs[<span class="string">&quot;token_type_ids&quot;</span>] = (batch[<span class="number">2</span>] <span class="keyword">if</span> model.config.model_type <span class="keyword">in</span> [<span class="string">&quot;bert&quot;</span>, <span class="string">&quot;xlnet&quot;</span>] </span><br><span class="line">                                    <span class="keyword">else</span> <span class="literal">None</span>)  <span class="comment"># XLM and RoBERTa don&quot;t use token_type_ids</span></span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        logits = outputs[-<span class="number">1</span>] </span><br><span class="line"><span class="keyword">return</span> logits, label_batch</span><br></pre></td></tr></table></figure>
<h2 id="get-rehearsal-prototype"><a href="#get-rehearsal-prototype" class="headerlink" title="get_rehearsal_prototype()"></a>get_rehearsal_prototype()</h2><h3 id="1-加载支持集以及它们的encodings和labels"><a href="#1-加载支持集以及它们的encodings和labels" class="headerlink" title="1. 加载支持集以及它们的encodings和labels"></a>1. 加载支持集以及它们的encodings和labels</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">support_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;memory&quot;</span>, data_dir=data_dir)</span><br><span class="line">support_o_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;memory_o&quot;</span>, data_dir=data_dir)</span><br><span class="line">support_sampler = SequentialSampler(support_dataset)</span><br><span class="line">support_dataloader = DataLoader(support_dataset, sampler=support_sampler, batch_size=args.eval_batch_size)</span><br><span class="line">support_o_dataloader = DataLoader(support_o_dataset, sampler=support_o_sampler, batch_size=args.eval_batch_size)</span><br><span class="line">support_encodings, support_labels = get_support_features_and_labels(args, model, support_dataloader, support_o_dataloader, pad_token_label_id)</span><br><span class="line"><span class="comment"># 将support_encodings归一化</span></span><br><span class="line">support_encodings = F.normalize(support_encodings)</span><br></pre></td></tr></table></figure>
<h3 id="2-计算类别相似度"><a href="#2-计算类别相似度" class="headerlink" title="2. 计算类别相似度"></a>2. 计算类别相似度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(labels)):  <span class="comment"># 迭代每个非&quot;O&quot;标签的类别</span></span><br><span class="line">    <span class="comment"># 计算类别i的样本之间的余弦相似度</span></span><br><span class="line">    support_reps_dists = torch.matmul(support_encodings[support_labels == i],</span><br><span class="line">                                          support_encodings[support_labels == i].T)</span><br><span class="line">    <span class="comment"># 将对角线上的元素（样本与自身的相似度）设置为0，以避免将自身视为原型。</span></span><br><span class="line">    support_reps_dists = torch.scatter(support_reps_dists, <span class="number">1</span>, torch.arange(support_reps_dists.shape[<span class="number">0</span>]).view(-<span class="number">1</span>, <span class="number">1</span>).to(args.device),<span class="number">0.</span>)</span><br><span class="line">    <span class="comment"># 计算类别i的类别相似度</span></span><br><span class="line">    prototype_dists.append(support_reps_dists[support_reps_dists &gt; <span class="number">0</span>].view(-<span class="number">1</span>).mean(-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="3-返回类别相似度列表"><a href="#3-返回类别相似度列表" class="headerlink" title="3. 返回类别相似度列表"></a>3. 返回类别相似度列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> prototype_dists</span><br></pre></td></tr></table></figure>


<h1 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h1><h2 id="MySftBertModel"><a href="#MySftBertModel" class="headerlink" title="MySftBertModel()"></a>MySftBertModel()</h2><h3 id="1-初始化-init"><a href="#1-初始化-init" class="headerlink" title="1. 初始化 init"></a>1. 初始化 init</h3><p>接受Bert配置参数以及其他自定义参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.per_types = per_types  <span class="comment"># 设置每轮任务的类型数量。</span></span><br><span class="line">self.feat_dim = feat_dim  <span class="comment"># 设置特征维度。</span></span><br><span class="line">self.hidden_size = config.hidden_size  <span class="comment"># 设置隐藏状态的大小。</span></span><br><span class="line">self.num_labels = config.num_labels  <span class="comment"># 设置标签数量。</span></span><br><span class="line">self.bert = BertModel(config, add_pooling_layer=<span class="literal">False</span>) </span><br></pre></td></tr></table></figure>
<p>设置了分类器(classifier)和投影头(head)，根据mode选择性地设置分类器的输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">classifier_dropout = (  <span class="comment"># 设置分类器的dropout概率</span></span><br><span class="line">            config.classifier_dropout <span class="keyword">if</span> config.classifier_dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> config.hidden_dropout_prob</span><br><span class="line">        )</span><br><span class="line"><span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:  <span class="comment"># 根据不同模式设置线性分类器的不同输出维度</span></span><br><span class="line">    <span class="keyword">if</span> self.num_labels-<span class="number">1</span> &gt; self.per_types:  <span class="comment"># 对“O”样本重新标记过</span></span><br><span class="line">        self.classifier = nn.Linear(config.hidden_size, config.num_labels - self.per_types)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.classifier = nn.Linear(config.hidden_size, config.num_labels)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.classifier = nn.Linear(config.hidden_size, config.num_labels)</span><br></pre></td></tr></table></figure>
<p>设置不同的head（线性层或多层感知机）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> head == <span class="string">&#x27;linear&#x27;</span>:</span><br><span class="line">   self.head = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line"><span class="keyword">elif</span> head == <span class="string">&#x27;mlp&#x27;</span>:</span><br><span class="line">    self.head = nn.Sequential(</span><br><span class="line">        nn.Linear(self.hidden_size, self.hidden_size),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(self.hidden_size, self.feat_dim)</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&#x27;head not supported: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(head))</span><br></pre></td></tr></table></figure>
<h3 id="2-前向传播-forward"><a href="#2-前向传播-forward" class="headerlink" title="2. 前向传播 forward"></a>2. 前向传播 forward</h3><p><strong>提取特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用原有的Bert模型初步提取出样本的特征</span></span><br><span class="line">outputs = self.bert(...)</span><br><span class="line">features_enc = outputs[<span class="number">0</span>]   </span><br><span class="line"><span class="comment"># 通过self.head对进一步提取出样本的特征并归一化。</span></span><br><span class="line">features = F.normalize(self.head(features_enc.view(-<span class="number">1</span>, self.hidden_size)), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 使用初步特征进行预测</span></span><br><span class="line">sequence_output = outputs[<span class="number">0</span>]</span><br><span class="line">sequence_output = self.dropout(sequence_output)</span><br><span class="line">logits = self.classifier(sequence_output)</span><br></pre></td></tr></table></figure>
<p>如果不是训练模式，直接返回features_enc，features，logits</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> mode != <span class="string">&quot;train&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> loss, features_enc, features, logits</span><br></pre></td></tr></table></figure>
<p><strong>计算损失函数</strong><br>如果是第一轮训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.num_labels-<span class="number">1</span> == self.per_types:  </span><br><span class="line">    <span class="keyword">if</span> loss_name == <span class="string">&quot;supcon&quot;</span>:         loss = supcon_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o&quot;</span>:     loss = supcon_o_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o_ce&quot;</span>:  loss = supcon_o_loss+ce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o_bce&quot;</span>: loss = supcon_o_loss + bce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;ce&quot;</span>:           loss = ce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;bce_o&quot;</span>:        loss = bce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_ce&quot;</span>:    loss = supcon_loss + ce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_bce&quot;</span>:   loss = supcon_loss + bce_loss</span><br></pre></td></tr></table></figure>
<p>如果不是第一轮训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> self.num_labels &gt; self.per_types: </span><br><span class="line">    <span class="comment"># 整理新类别标签labels_new，新类别样本的logits：student_new 以及</span></span><br><span class="line">    <span class="comment"># 旧类别样本的logits：s_logits，teacher模型的logits：old_logits</span></span><br><span class="line">    labels_new, student_new, s_logits, old_logits = gather_rh_ce( labels, t_logits, </span><br><span class="line">                                            logits, self.num_labels - self.per_types)</span><br><span class="line">    <span class="keyword">if</span> loss_name == <span class="string">&quot;supcon&quot;</span>:        loss = supcon_loss+kd_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_nokd&quot;</span>: loss = supcon_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o&quot;</span>:    loss = supcon_o_loss+kd_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o_ce&quot;</span>: loss = supcon_o_loss+ce_loss+kd_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_o_bce&quot;</span>:loss = supcon_o_loss + bce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;ce&quot;</span>:          loss = ce_loss+kd_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;bce_o&quot;</span>:       loss = bce_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_ce&quot;</span>:   loss = supcon_loss+ce_loss+kd_loss</span><br><span class="line">    <span class="keyword">elif</span> loss_name == <span class="string">&quot;supcon_bce&quot;</span>:  loss = supcon_loss+bce_loss</span><br></pre></td></tr></table></figure>
<h3 id="3-返回-loss-features-enc-features-logits"><a href="#3-返回-loss-features-enc-features-logits" class="headerlink" title="3. 返回 loss, features_enc, features, logits"></a>3. 返回 loss, features_enc, features, logits</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> loss, features_enc, features, logits</span><br></pre></td></tr></table></figure>


<h1 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h1><h2 id="train-and-eval"><a href="#train-and-eval" class="headerlink" title="train_and_eval( )"></a>train_and_eval( )</h2><h3 id="1-加载上一轮预训练的参数配置、模型和分词器"><a href="#1-加载上一轮预训练的参数配置、模型和分词器" class="headerlink" title="1. 加载上一轮预训练的参数配置、模型和分词器"></a>1. 加载上一轮预训练的参数配置、模型和分词器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建模型配置、模型类别和分词器类</span></span><br><span class="line">config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]</span><br><span class="line"><span class="comment"># 如果是第一轮，则直接加载 bert-base-uncased 模型</span></span><br><span class="line">config = config_class.from_pretrained(args.config_name <span class="keyword">if</span> args.config_name <span class="keyword">else</span> model_name_or_path，num_labels=num_labels)</span><br><span class="line">tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name <span class="keyword">if</span> args.tokenizer_name <span class="keyword">else</span> model_name_or_path,do_lower_case=args.do_lower_case)</span><br><span class="line">model = model_class.from_pretrained(model_name_or_path, from_tf=<span class="built_in">bool</span>(<span class="string">&quot;.ckpt&quot;</span> <span class="keyword">in</span> model_name_or_path),config=config)</span><br></pre></td></tr></table></figure>
<h3 id="2-获取训练集"><a href="#2-获取训练集" class="headerlink" title="2. 获取训练集"></a>2. 获取训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_dataset=TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)</span></span><br><span class="line">train_dataset=load_and_cache_examples(args,tokenizer,labels,pad_token_label_id,mode=<span class="string">&quot;rehearsal&quot;</span>,data_dir=data_dir)</span><br><span class="line"><span class="comment"># 顺序采样</span></span><br><span class="line">train_sampler = SequentialSampler(train_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(train_dataset)</span><br><span class="line"><span class="comment"># 创建训练数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)  </span><br></pre></td></tr></table></figure>
<h3 id="3-获取旧模型的特征"><a href="#3-获取旧模型的特征" class="headerlink" title="3. 获取旧模型的特征"></a>3. 获取旧模型的特征</h3><p><strong>teacher_evaluate()</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果当前不是第一个任务，则需要对老师模型进行评估。</span></span><br><span class="line"><span class="keyword">if</span> step_id &gt; <span class="number">0</span>: </span><br><span class="line">    t_logits, out_new_labels = teacher_evaluate(args, train_dataloader, model, tokenizer,labels,</span><br><span class="line">                                                pad_token_label_id, mode=<span class="string">&quot;train&quot;</span>, data_dir=data_dir)</span><br><span class="line">    model.new_classifier()  <span class="comment"># 创建一个新的分类器</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    t_logits = <span class="literal">None</span></span><br><span class="line">    out_new_labels = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="4-训练模型"><a href="#4-训练模型" class="headerlink" title="4. 训练模型"></a>4. 训练模型</h3><p><strong>train()</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">global_step, tr_loss = train(args, train_dataset, train_dataloader, model, tokenizer, labels,</span><br><span class="line">                                pad_token_label_id, data_dir=data_dir, output_dir=output_dir,</span><br><span class="line">                                t_logits=t_logits, out_new_labels=out_new_labels)</span><br><span class="line"><span class="comment"># 保存训练过程中得到的模型参数、配置和分词器    </span></span><br><span class="line">model_to_save.save_pretrained(output_dir)</span><br><span class="line">tokenizer.save_pretrained(output_dir)</span><br><span class="line">torch.save(args, os.path.join(output_dir, <span class="string">&quot;training_args.bin&quot;</span>))                            </span><br></pre></td></tr></table></figure>
<h3 id="5-在开发集上评估模型"><a href="#5-在开发集上评估模型" class="headerlink" title="5.在开发集上评估模型"></a>5.在开发集上评估模型</h3><p><strong>evaluate()</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于每个检查点，加载模型并进行评估</span></span><br><span class="line"><span class="keyword">for</span> checkpoint <span class="keyword">in</span> checkpoints:</span><br><span class="line">    model = model_class.from_pretrained(checkpoint, mode=<span class="string">&quot;dev&quot;</span>)</span><br><span class="line">    train_dataloader=<span class="literal">None</span></span><br><span class="line">     _, result,  _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;dev&quot;</span>,</span><br><span class="line">                            data_dir=data_dir, prefix=global_step)</span><br></pre></td></tr></table></figure>
<h3 id="6-在测试集上进行预测"><a href="#6-在测试集上进行预测" class="headerlink" title="6.在测试集上进行预测"></a>6.在测试集上进行预测</h3><p><strong>evaluate()</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型和分词器</span></span><br><span class="line">tokenizer = tokenizer_class.from_pretrained(output_dir, do_lower_case=args.do_lower_case)</span><br><span class="line">model = model_class.from_pretrained(output_dir, mode=<span class="string">&quot;test&quot;</span>)</span><br><span class="line"><span class="comment"># 调用 evaluate 函数对测试集进行预测，获取macro-F1和micro-F1结果以及预测的标签。</span></span><br><span class="line">macro_results, micro_results, predictions = evaluate(args, model, tokenizer, labels,</span><br><span class="line">                                                    pad_token_label_id, mode=<span class="string">&quot;test&quot;</span>, data_dir=data_dir)</span><br></pre></td></tr></table></figure>

<h2 id="teacher-evaluate"><a href="#teacher-evaluate" class="headerlink" title="teacher_evaluate()"></a>teacher_evaluate()</h2><h3 id="1-根据不同模式设置数据集加载器"><a href="#1-根据不同模式设置数据集加载器" class="headerlink" title="1. 根据不同模式设置数据集加载器"></a>1. 根据不同模式设置数据集加载器</h3><p>如果模式是 “train”，，则使用训练数集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">    eval_dataloader = train_dataloader</span><br></pre></td></tr></table></figure>
<p>如果模式是 “dev”，则使用开发集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> mode == <span class="string">&quot;dev&quot;</span>:</span><br><span class="line">    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode,data_dir=data_dir)</span><br><span class="line">    eval_sampler = SequentialSampler(eval_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(eval_dataset)</span><br><span class="line">    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="2-评估模型"><a href="#2-评估模型" class="headerlink" title="2. 评估模型"></a>2. 评估模型</h3><p>将模型设置为评估模式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>使用 get_token_logits_and_labels 函数获取每个batch的预测分数 logits 和输出标签 out_labels</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(eval_dataloader, desc=<span class="string">&quot;Evaluating&quot;</span>):</span><br><span class="line">    logits, out_labels = get_token_logits_and_labels(args, model, batch)</span><br><span class="line">    <span class="comment"># 对评估步骤计数，以便跟踪已评估的批次数量</span></span><br><span class="line">    nb_eval_steps += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将每个批次的 logits 分数添加到 logits_list 列表中</span></span><br><span class="line">    logits_list.append(logits.detach().cpu())</span><br></pre></td></tr></table></figure>
<h3 id="3-用原型重新标记阈值重新标记旧实体类"><a href="#3-用原型重新标记阈值重新标记旧实体类" class="headerlink" title="3. 用原型重新标记阈值重新标记旧实体类"></a>3. 用原型重新标记阈值重新标记旧实体类</h3><p>计算原型重新标记阈值和与每个样本的原型相似度最高的实体类别<br><strong>evaluate()</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算每个批次样本中原型相似度最大值所在的类别索引 preds</span></span><br><span class="line"><span class="comment"># 每个批次样本与每个类别的原型相似度的最大值 emissions</span></span><br><span class="line"><span class="comment"># 根据原有模型预测的标签索引序列 out_label_ids</span></span><br><span class="line"><span class="comment"># 每个旧类别的原型重新标记阈值列表（还未乘βi）prototype_dists</span></span><br><span class="line">preds, emissions, out_label_ids, prototype_dists = evaluate(args, model, tokenizer, labels, pad_token_label_id, </span><br><span class="line">                                                        mode=<span class="string">&quot;rehearsal&quot;</span>, data_dir=data_dir)</span><br><span class="line"><span class="comment"># 计算原型重新标记阈值 (根据不同的任务步骤i来调整超参数βi)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(current_task_id):</span><br><span class="line">    <span class="keyword">if</span> args.change_th:</span><br><span class="line">        task_para = th_para - (current_task_id - i - <span class="number">1</span>)*th_reduction  <span class="comment"># βi</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        task_para = th_para</span><br><span class="line">        prototype_dists[i*args.per_types+<span class="number">1</span>:(i+<span class="number">1</span>)*args.per_types+<span class="number">1</span>] *= task_para</span><br></pre></td></tr></table></figure>
<p>重新标记旧实体类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out_label_ids.shape[<span class="number">0</span>]):  <span class="comment"># 迭代每个batch  </span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(out_label_ids.shape[<span class="number">1</span>]):  <span class="comment"># 迭代每个样本  </span></span><br><span class="line">        idx = preds[i][j]   <span class="comment"># 根据原型相似度预测的类别索引</span></span><br><span class="line">        <span class="comment"># 如果原型的相似度大于重新标记阈值并且预测的标签是旧实体类的标签</span></span><br><span class="line">        <span class="keyword">if</span> emissions[i][j] &gt; prototype_dists[idx].item() <span class="keyword">and</span> out_label_ids[i][j] &lt; <span class="built_in">len</span>(labels) - args.per_types: </span><br><span class="line">                out_label_new_list[i].append(preds[i][j])  <span class="comment"># 则将该“O”预测为这个旧实体类  </span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 否则，保持原始的标签不变</span></span><br><span class="line">            out_label_new_list[i].append(out_label_ids[i][j])</span><br></pre></td></tr></table></figure>
<h3 id="4-返回-logits-list-out-label-new-list"><a href="#4-返回-logits-list-out-label-new-list" class="headerlink" title="4. 返回 logits_list, out_label_new_list"></a>4. 返回 logits_list, out_label_new_list</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> logits_list, out_label_new_list</span><br></pre></td></tr></table></figure>


<h2 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate ()"></a>evaluate ()</h2><h3 id="1-读取数据集"><a href="#1-读取数据集" class="headerlink" title="1. 读取数据集"></a>1. 读取数据集</h3><p>读取eval_dataset，support_dataset，support_o_dataset，train_dataset。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode, data_dir=data_dir)</span><br><span class="line">support_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;memory&quot;</span>, data_dir=data_dir)</span><br><span class="line">support_o_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;memory_o&quot;</span>, data_dir=data_dir)</span><br><span class="line">train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;train&quot;</span>, data_dir=data_dir)</span><br><span class="line"><span class="comment"># 顺序采样</span></span><br><span class="line">eval_sampler = SequentialSampler(eval_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(eval_dataset)</span><br><span class="line">eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)</span><br><span class="line">support_sampler = SequentialSampler(support_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(support_dataset)</span><br><span class="line">support_o_sampler = SequentialSampler(support_o_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(support_o_dataset)</span><br><span class="line">train_sampler = SequentialSampler(train_dataset) <span class="keyword">if</span> args.local_rank == -<span class="number">1</span> <span class="keyword">else</span> DistributedSampler(train_dataset)</span><br><span class="line"><span class="comment"># 数据集加载器</span></span><br><span class="line">support_dataloader = DataLoader(support_dataset, sampler=support_sampler, batch_size=args.eval_batch_size)</span><br><span class="line">support_o_dataloader = DataLoader(support_o_dataset, sampler=support_o_sampler, batch_size=args.eval_batch_size)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.eval_batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="2-获取支持数据集的embbedings和labels"><a href="#2-获取支持数据集的embbedings和labels" class="headerlink" title="2. 获取支持数据集的embbedings和labels"></a>2. 获取支持数据集的embbedings和labels</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">support_encodings, support_labels = get_support_encodings_and_labels_total(args, model, support_dataloader, support_o_dataloader, train_dataloader, pad_token_label_id)</span><br></pre></td></tr></table></figure>
<h3 id="3-三种重新标记来自旧类别的“O”的策略"><a href="#3-三种重新标记来自旧类别的“O”的策略" class="headerlink" title="3. 三种重新标记来自旧类别的“O”的策略"></a>3. 三种重新标记来自旧类别的“O”的策略</h3><h4 id="1）使用原型重新标记"><a href="#1）使用原型重新标记" class="headerlink" title="1）使用原型重新标记"></a>1）使用原型重新标记</h4><p>基于“O”样本与原型之间的距离<br><strong>计算每个类别的原型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exemplar_means = get_exemplar_means(args, support_encodings, support_labels)</span><br></pre></td></tr></table></figure>
<p><strong>计算原型重新标记阈值以及“O”与原型的最高相似度</strong></p>
<p>利用 NNClassification() 计算nn_preds，nn_emissions，prototype_dists</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_iterator):</span><br><span class="line">    batch = <span class="built_in">tuple</span>(t.to(args.device) <span class="keyword">for</span> t <span class="keyword">in</span> batch)</span><br><span class="line">    <span class="comment"># 循环迭代 eval_iterator，使用原有的模型获取每个批次的encodings和labels</span></span><br><span class="line">    encodings, encoding_labels = get_token_encodings_and_labels(args, model, batch)</span><br><span class="line">    <span class="comment"># 如果是rehearsal模式，则去除掉当前task样本的support_encodings和support_labels再进行预测。</span></span><br><span class="line">    <span class="keyword">if</span> mode==<span class="string">&quot;rehearsal&quot;</span>:</span><br><span class="line">        cls = NNClassification()</span><br><span class="line">        support_encodings = support_encodings[support_labels &lt; <span class="built_in">len</span>(labels) - args.per_types]</span><br><span class="line">        support_labels = support_labels[support_labels &lt; <span class="built_in">len</span>(labels) - args.per_types]</span><br><span class="line">        nn_preds(batch_size, sent_len) 包含每个样本中原型相似度最大值所在的类别索引</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn_preds(batch_size, sent_len) 包含每个样本中原型相似度最大值所在的类别索引</span></span><br><span class="line"><span class="string">        nn_emissions(batch_size, sent_len, ndim) 包含每个样本与每个类别的原型相似度的最大值</span></span><br><span class="line"><span class="string">        prototype_dists 每个旧类别的原型重新标记阈值列表（还未乘βi）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nn_preds, nn_emissions, prototype_dists = cls.nn_classifier_dot_prototype(encodings, support_encodings, support_labels, exemplar_means)</span><br></pre></td></tr></table></figure>
<h4 id="2）使用最近邻重新标记"><a href="#2）使用最近邻重新标记" class="headerlink" title="2）使用最近邻重新标记"></a>2）使用最近邻重新标记</h4><p>基于“O”样本与每个类别示例之间的距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.cls_name == <span class="string">&quot;ncm_dot&quot;</span>:  </span><br><span class="line">    cls = NcmClassification()</span><br><span class="line">    nn_preds = cls.ncm_classifier_dot(encodings, support_encodings, support_labels, exemplar_means)</span><br></pre></td></tr></table></figure>
<h4 id="3）使用原有模型重新标记"><a href="#3）使用原有模型重新标记" class="headerlink" title="3）使用原有模型重新标记"></a>3）使用原有模型重新标记</h4><p>作为前两种方法的参考标注</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> args.cls_name == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">    nn_preds, encoding_labels = get_token_logits_and_labels(args, model, batch)</span><br></pre></td></tr></table></figure>
<h3 id="4-保存预测结果"><a href="#4-保存预测结果" class="headerlink" title="4. 保存预测结果"></a>4. 保存预测结果</h3><p>将每个批次的预测结果追加到 preds 中，并将作为参考标准的预测标签保存到 out_label_ids 中。如果当前模式是rehearsal模式，还会保存emissions。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> preds <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 第一次预测</span></span><br><span class="line">    preds = nn_preds.detach().cpu().numpy()</span><br><span class="line">    out_label_ids = encoding_labels.detach().cpu().numpy()</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;rehearsal&quot;</span>:</span><br><span class="line">        emissions = nn_emissions.detach().cpu().numpy()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    preds = np.append(preds, nn_preds.detach().cpu().numpy(), axis=<span class="number">0</span>)</span><br><span class="line">    out_label_ids = np.append(out_label_ids, encoding_labels.detach().cpu().numpy(), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&quot;rehearsal&quot;</span>:</span><br><span class="line">        emissions = np.append(emissions, nn_emissions.detach().cpu().numpy(), axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-预测结果"><a href="#5-预测结果" class="headerlink" title="5. 预测结果"></a>5. 预测结果</h3><p>如果当前模式是rehearsal模式，那么函数直接将返回 preds、 emissions、out_label_ids 和prototype_dists。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">&quot;rehearsal&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> preds, emissions, out_label_ids, prototype_dists</span><br></pre></td></tr></table></figure>
<p>如果使用的是线性分类器，根据 preds 得出预测的最大logits？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.cls_name == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">    preds = np.argmax(preds, axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>out_label_list 和 preds_list存储使用原有模型和使用自定义方法预测的标签字符串序列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 label_map 字典，将标签的索引映射到相应标签的字符串名称。</span></span><br><span class="line">label_map = &#123;i: <span class="string">&quot;I-&quot;</span>+label <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels)&#125;</span><br><span class="line">label_map[<span class="number">0</span>] = <span class="string">&quot;O&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out_label_ids.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(out_label_ids.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> out_label_ids[i, j] != pad_token_label_id:</span><br><span class="line">            out_label_list[i].append(label_map[out_label_ids[i][j]])</span><br><span class="line">            preds_list[i].append(label_map[preds[i][j]])</span><br></pre></td></tr></table></figure>
<p>输出评价指标结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 seqeval 库计算 F1-score。</span></span><br><span class="line">metric = load_metric(<span class="string">&quot;seqeval&quot;</span>)</span><br><span class="line">metric.add_batch(predictions=preds_list, references=out_label_list)</span><br><span class="line">macro_results, micro_results, _ = compute_metrics(metric)</span><br></pre></td></tr></table></figure>
<p>返回评价指标结果以及预测的标签序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> macro_results, micro_results, preds_list</span><br></pre></td></tr></table></figure>
<h2 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h2><h3 id="1-计算训练总步数t-total和训练轮数num-train-epochs"><a href="#1-计算训练总步数t-total和训练轮数num-train-epochs" class="headerlink" title="1. 计算训练总步数t_total和训练轮数num_train_epochs"></a>1. 计算训练总步数t_total和训练轮数num_train_epochs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果设置了训练最大步数max_steps，则t_total = args.max_steps，并计算num_train_epochs，否则根据num_train_epochs计算t_total。</span></span><br><span class="line"><span class="keyword">if</span> args.max_steps &gt; <span class="number">0</span>:</span><br><span class="line">    t_total = args.max_steps</span><br><span class="line">    args.num_train_epochs = args.max_steps // (<span class="built_in">len</span>(train_dataloader) // args.gradient_accumulation_steps) + <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    t_total = <span class="built_in">len</span>(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs</span><br></pre></td></tr></table></figure>
<h3 id="2-配置优化器"><a href="#2-配置优化器" class="headerlink" title="2. 配置优化器"></a>2. 配置优化器</h3><p>使用AdamW优化器，使用了。权重衰减和学习率调节器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">no_decay = [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;LayerNorm.weight&quot;</span>]  <span class="comment"># 不需要衰减的参数</span></span><br><span class="line">optimizer_grouped_parameters = [</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)],</span><br><span class="line">    <span class="string">&quot;weight_decay&quot;</span>: args.weight_decay&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">]</span><br><span class="line">optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)</span><br><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)</span><br></pre></td></tr></table></figure>
<h3 id="3-训练"><a href="#3-训练" class="headerlink" title="3. 训练"></a>3. 训练</h3><p>迭代每一轮</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> train_iterator:</span><br><span class="line">    <span class="keyword">if</span> epoch &gt;= args.start_train_o_epoch:</span><br></pre></td></tr></table></figure>
<p>获取每个类别的类别相似度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prototype_dists = get_rehearsal_prototype(args, model, tokenizer, labels,</span><br><span class="line">                                   pad_token_label_id, mode=<span class="string">&quot;rehearsal&quot;</span>,</span><br><span class="line">                                   data_dir=data_dir)</span><br></pre></td></tr></table></figure>
<p>获取样本的logits和标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按批次遍历训练集中的数据</span></span><br><span class="line"><span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(epoch_iterator):</span><br><span class="line">    model.train()  <span class="comment"># 将模型切换到训练模式</span></span><br><span class="line">    <span class="keyword">if</span> num_labels-<span class="number">1</span> &gt; args.per_types:  <span class="comment"># 如果不是第一轮训练</span></span><br><span class="line">        t_logits_step = t_logits[step]</span><br><span class="line">        new_labels = out_new_labels[step * args.train_batch_size:step * args.train_batch_size + <span class="built_in">len</span>(batch[<span class="number">3</span>])]</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 如果是第一轮训练，使用训练集的原始标签</span></span><br><span class="line">        t_logits_step = <span class="literal">None</span></span><br><span class="line">        new_labels = batch[<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">if</span> epoch &gt;= args.start_train_o_epoch:</span><br><span class="line">        loss_name = args.loss_name2  <span class="comment"># 实体和“O”的联合损失函数</span></span><br><span class="line">        cls = NNClassification()</span><br><span class="line">        encodings, encoding_labels = get_token_features_and_labels(args, model, batch)</span><br></pre></td></tr></table></figure>
<p>计算样本之间的余弦相似度分数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    # top_emissions_step(batch_size*set_len, batch_size*set_len):存储样本之间大于实体阈值的余弦相似度分数</span><br><span class="line">    # 选择类别相似度的中位数作为实体阈值 th_dists</span><br><span class="line">    top_emissions_step, _ = cls.get_top_emissions_with_th(encodings, encoding_labels, </span><br><span class="line">                                                        th_dists=torch.median(prototype_dists).item())</span><br><span class="line">else:</span><br><span class="line">    top_emissions_step = top_emissions</span><br></pre></td></tr></table></figure>
<p>用自定义的模型进行训练，获取损失值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">inputs = &#123;<span class="string">&quot;input_ids&quot;</span>: batch[<span class="number">0</span>],  </span><br><span class="line">        <span class="string">&quot;attention_mask&quot;</span>: batch[<span class="number">1</span>],  </span><br><span class="line">        <span class="string">&quot;token_type_ids&quot;</span>: batch[<span class="number">2</span>] <span class="keyword">if</span> args.model_type <span class="keyword">in</span> [<span class="string">&quot;bert&quot;</span>, <span class="string">&quot;xlnet&quot;</span>] <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">        <span class="comment"># XLM and RoBERTa don&quot;t use segment_ids</span></span><br><span class="line">        <span class="string">&quot;labels&quot;</span>: new_labels,  </span><br><span class="line">        <span class="string">&quot;t_logits&quot;</span>: t_logits_step,  </span><br><span class="line">        <span class="string">&quot;mode&quot;</span>: <span class="string">&quot;train&quot;</span>,  </span><br><span class="line">        <span class="string">&quot;loss_name&quot;</span>: loss_name,  </span><br><span class="line">        <span class="string">&quot;top_emissions&quot;</span>: top_emissions_step,</span><br><span class="line">        <span class="string">&quot;topk_th&quot;</span>: <span class="literal">True</span>  </span><br><span class="line">        &#125;</span><br><span class="line">outputs = model(**inputs)  </span><br><span class="line">loss = outputs[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果设置了梯度累积步数，则需要对损失值进行除以梯度累积步数，以得到平均损失值</span></span><br><span class="line"><span class="keyword">if</span> args.gradient_accumulation_steps &gt; <span class="number">1</span>:</span><br><span class="line">    loss = loss / args.gradient_accumulation_steps</span><br></pre></td></tr></table></figure>
<p>更新参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()  <span class="comment"># 后向传播</span></span><br><span class="line">tr_loss += loss.item()</span><br><span class="line"><span class="comment"># 如果达到梯度累计步数</span></span><br><span class="line"><span class="keyword">if</span> (step + <span class="number">1</span>) % args.gradient_accumulation_steps == <span class="number">0</span>:</span><br><span class="line">    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) </span><br><span class="line">    optimizer.step()  </span><br><span class="line">    scheduler.step()  </span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 清除优化器中所有参数的梯度</span></span><br><span class="line">    global_step += <span class="number">1</span>  <span class="comment"># 更新全局步数</span></span><br></pre></td></tr></table></figure>
<h3 id="4-评估"><a href="#4-评估" class="headerlink" title="4. 评估"></a>4. 评估</h3><p>在开发集上评估模型性能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_, results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=<span class="string">&quot;dev&quot;</span>,</span><br><span class="line">                                     data_dir=data_dir)</span><br></pre></td></tr></table></figure>
<p>保存模型及参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_to_save.save_pretrained(output_dir)</span><br><span class="line">torch.save(args, os.path.join(output_dir, <span class="string">&quot;training_args.bin&quot;</span>)) </span><br></pre></td></tr></table></figure>
<p>达到最大步数时停止训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if args.max_steps &gt; 0 and global_step &gt; args.max_steps:  </span><br><span class="line">    epoch_iterator.close()  # 关闭当前的 epoch 迭代器</span><br><span class="line">    break</span><br></pre></td></tr></table></figure>
<h3 id="5-返回全局步数和平均损失"><a href="#5-返回全局步数和平均损失" class="headerlink" title="5. 返回全局步数和平均损失"></a>5. 返回全局步数和平均损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> global_step, tr_loss / global_step  </span><br></pre></td></tr></table></figure>


<h1 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h1><h2 id="NNClassification"><a href="#NNClassification" class="headerlink" title="NNClassification()"></a>NNClassification()</h2><h3 id="nn-classifier-dot-prototype"><a href="#nn-classifier-dot-prototype" class="headerlink" title="nn_classifier_dot_prototype()"></a>nn_classifier_dot_prototype()</h3><p><strong>根据原型进行重新标记来自旧实体类别的“O”</strong><br>计算“O”与每个类别原型的最大相似度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将输入的表示 reps 重塑为二维张量，并对其进行归一化处理。</span></span><br><span class="line">feature = reps.view(-<span class="number">1</span>, reps.shape[-<span class="number">1</span>])  <span class="comment"># (batch_size, ndim)</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(feature.size(<span class="number">0</span>)):  <span class="comment"># Normalize</span></span><br><span class="line">    feature.data[j] = feature.data[j] / feature.data[j].norm()</span><br><span class="line">    means = torch.stack([exemplar_means[cls] <span class="keyword">for</span> cls <span class="keyword">in</span> <span class="built_in">range</span>(n_tags)])  <span class="comment"># (n_classes, ndim)</span></span><br><span class="line">    dists = torch.matmul(feature, means.T)  <span class="comment"># (batch_size, n_classes)</span></span><br><span class="line">    dists[:, <span class="number">0</span>] = torch.zeros(<span class="number">1</span>).to(reps.device)  <span class="comment"># 将第一列真正的“O” 类别的相似度设为0</span></span><br><span class="line">    <span class="comment"># emissions 包含每个样本中原型相似度的最大值，tags 包含每个样本中原型相似度最大值所在的类别索引。</span></span><br><span class="line">    emissions, tags = dists.<span class="built_in">max</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>计算每个类别的原型重新标记阈值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将support_reps 重塑为二维张量，并对其进行归一化处理。</span></span><br><span class="line">support_reps = support_reps.view(-<span class="number">1</span>, support_reps.shape[-<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(support_reps.size(<span class="number">0</span>)):  <span class="comment"># Normalize</span></span><br><span class="line">    support_reps.data[j] = support_reps.data[j] / support_reps.data[j].norm()</span><br><span class="line">support_reps = F.normalize(support_reps)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_tags):</span><br><span class="line">    <span class="comment"># 计算每个类别原型与支持集中对应类别的样本的相似度</span></span><br><span class="line">    support_reps_dists = torch.matmul(support_reps[support_tags==i], means[i].T)</span><br><span class="line">    <span class="comment"># 沿着最后一个维度（即特征维度）寻找最小值，并返回这些最小值以及对应的索引</span></span><br><span class="line">    prototype_dists.append(support_reps_dists.<span class="built_in">min</span>(-<span class="number">1</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h3 id="get-top-emissions-with-th"><a href="#get-top-emissions-with-th" class="headerlink" title="get_top_emissions_with_th()"></a>get_top_emissions_with_th()</h3><p>计算样本之间的余弦相似度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scores = self._euclidean_metric_dot_2(reps.view(-<span class="number">1</span>, ndim), reps.view(-<span class="number">1</span>, ndim), <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 排除“O”样本的分数（第二维）</span></span><br><span class="line">        scores = torch.where(reps_labels == <span class="number">0</span>, scores.double(), -<span class="number">100.</span>)  </span><br><span class="line">        <span class="comment"># 排除样本与自身的分数</span></span><br><span class="line">        scores = torch.scatter(scores, <span class="number">1</span>,</span><br><span class="line">                               torch.arange(scores.shape[<span class="number">0</span>]).view(-<span class="number">1</span>, <span class="number">1</span>).to(device), -<span class="number">100.</span>)</span><br></pre></td></tr></table></figure>
<p>筛选出大于实体阈值的分数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top_emissions = scores &gt; th_dists</span><br></pre></td></tr></table></figure>
<p>返回 top_emissions, scores</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> top_emissions, scores</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://siljing.github.io/2024/03/22/Learing_o/" data-id="clu2o6bwy0000r4vr8o6rg0rn" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/22/Learing_o/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>